# -*- coding: utf-8 -*-
"""ml_pipeline_dag_v1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MubhAA-3iyKSMjtVNSIkpr99KYt9qUpd
"""

import datetime
import json
import logging

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.google.cloud.operators.bigquery import BigQueryInsertJobOperator
from airflow.operators.email import EmailOperator

from google.cloud import bigquery, storage
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
import joblib

default_args = {
    'owner': 'Osman El-Alawa',
    'depends_on_past': False,
    'start_date': datetime.datetime(2025, 8, 17),
    'retries': 1,
    'retry_delay': datetime.timedelta(minutes=5),
}

dag = DAG(
    'ml_pipeline_dag',
    default_args=default_args,
    description='Simple ML pipeline for bicycle duration prediction',
    schedule='0 0 * * 0',  # Weekly on Sunday at midnight
    catchup=False,
)

# Task 1: Data Extraction
sql_query = """
CREATE OR REPLACE TABLE `my-ml-project-469319.ml_dataset.cycle_hire_training_data` AS
SELECT
  duration AS target,
  EXTRACT(DAYOFWEEK FROM start_date) AS day_week,
  EXTRACT(HOUR FROM start_date) AS hour
FROM
  `bigquery-public-data.london_bicycles.cycle_hire`
WHERE
  duration IS NOT NULL
  AND start_date >= '2020-01-01'
LIMIT 10000;
"""

extract_data = BigQueryInsertJobOperator(
    task_id='extract_data',
    configuration={
        'query': {
            'query': sql_query,
            'useLegacySql': False,
        }
    },
    dag=dag,
)

# Task 2: Model Training & Persistence
def train_model(**context):
    bq_client = bigquery.Client()
    query = "SELECT * FROM `my-ml-project-469319.ml_dataset.cycle_hire_training_data`"
    df = bq_client.query(query).to_dataframe()

    X = df[['day_week', 'hour']]
    y = df['target']
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    model = LinearRegression()
    model.fit(X_train, y_train)
    preds = model.predict(X_test)
    mse = mean_squared_error(y_test, preds)
    metrics = {'mse': mse, 'samples': len(df)}

    logging.info(f"Model trained. MSE: {mse}")

    # Save model to GCS
    storage_client = storage.Client()
    bucket = storage_client.bucket('us-central1-ml-pipeline-env-dd338327-bucket')
    model_blob = bucket.blob('models/lin_reg_model_v1.joblib')
    joblib.dump(model, '/tmp/model.joblib')
    model_blob.upload_from_filename('/tmp/model.joblib')

    # Save metrics
    metrics_blob = bucket.blob('metrics/eval_metrics_v1.json')
    with open('/tmp/metrics.json', 'w') as f:
        json.dump(metrics, f)
    metrics_blob.upload_from_filename('/tmp/metrics.json')

    return json.dumps(metrics)

train_task = PythonOperator(
    task_id='train_model',
    python_callable=train_model,
    dag=dag,
)

# Task 3: Notification (Log)
def log_completion(**context):
    metrics = context['task_instance'].xcom_pull(task_ids='train_model')
    logging.info(f"ðŸŽ‰ Model training complete! Metrics: {metrics}. Check GCS for files.")

log_notify = PythonOperator(
    task_id='log_notify',
    python_callable=log_completion,
    dag=dag,
)

# Bonus: Email Notification
email_notify = EmailOperator(
    task_id='email_notify',
    to='elalawaosman@gmail.com',
    subject='ML Pipeline Complete',
    html_content="""<h3>Pipeline Success!</h3><p>Metrics: {{ ti.xcom_pull(task_ids='train_model') }}</p><p>Files in GCS: ml-pipeline-models</p>""",
    dag=dag,
)

# Dependencies
extract_data >> train_task >> log_notify >> email_notify